{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwjWXRnRkLlq"
      },
      "source": [
        "# Transfer Learning\r\n",
        "\r\n",
        "Most of the time you won't want to train a whole convolutional network yourself. Modern ConvNets training on huge datasets like ImageNet take weeks on multiple GPUs. Instead, most people use a pretrained network either as a fixed feature extractor, or as an initial network to fine tune. In this notebook, you'll be using [VGGNet](https://arxiv.org/pdf/1409.1556.pdf) trained on the [ImageNet dataset](http://www.image-net.org/) as a feature extractor. Below is a diagram of the VGGNet architecture.\r\n",
        "\r\n",
        "<img src=\"assets/cnnarchitecture.jpg\" width=700px>\r\n",
        "\r\n",
        "VGGNet is great because it's simple and has great performance, coming in second in the ImageNet competition. The idea here is that we keep all the convolutional layers, but replace the final fully connected layers with our own classifier. This way we can use VGGNet as a feature extractor for our images then easily train a simple classifier on top of that. What we'll do is take the first fully connected layer with 4096 units, including thresholding with ReLUs. We can use those values as a code for each image, then build a classifier on top of those codes.\r\n",
        "\r\n",
        "You can read more about transfer learning from [the CS231n course notes](http://cs231n.github.io/transfer-learning/#tf).\r\n",
        "\r\n",
        "## Pretrained VGGNet\r\n",
        "\r\n",
        "We'll be using a pretrained network from https://github.com/machrisaa/tensorflow-vgg. This code is already included in 'tensorflow_vgg' directory, sdo you don't have to clone it.\r\n",
        "\r\n",
        "This is a really nice implementation of VGGNet, quite easy to work with. The network has already been trained and the parameters are available from this link. **You'll need to clone the repo into the folder containing this notebook.** Then download the parameter file using the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsiR9htgkMHT",
        "outputId": "8ac70fe4-570f-4117-ef85-9d0516eba599"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn1Nwy7toKFa",
        "outputId": "ef48bea3-53ab-4a59-ebe2-2083b9ed8d8e"
      },
      "source": [
        "from urllib.request import urlretrieve\r\n",
        "from os.path import isfile, isdir\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "vgg_dir = 'tensorflow_vgg/'\r\n",
        "# Make sure vgg exists\r\n",
        "if not isdir(vgg_dir):\r\n",
        "    raise Exception(\"VGG directory doesn't exist!\")\r\n",
        "\r\n",
        "class DLProgress(tqdm):\r\n",
        "    last_block = 0\r\n",
        "\r\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\r\n",
        "        self.total = total_size\r\n",
        "        self.update((block_num - self.last_block) * block_size)\r\n",
        "        self.last_block = block_num\r\n",
        "\r\n",
        "if not isfile(vgg_dir + \"vgg16.npy\"):\r\n",
        "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:\r\n",
        "        urlretrieve(\r\n",
        "            'https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',\r\n",
        "            vgg_dir + 'vgg16.npy',\r\n",
        "            pbar.hook)\r\n",
        "else:\r\n",
        "    print(\"Parameter file already exists!\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG16 Parameters: 553MB [00:34, 16.1MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb4GU_4XonnD"
      },
      "source": [
        "## Flower power\r\n",
        "\r\n",
        "Here we'll be using VGGNet to classify images of flowers. To get the flower dataset, run the cell below. This dataset comes from the [TensorFlow inception tutorial](https://www.tensorflow.org/tutorials/image_retraining)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ2PLAULoYIE",
        "outputId": "0099df58-0ef3-46ba-b1d9-9ea5dae8931e"
      },
      "source": [
        "import tarfile\r\n",
        "\r\n",
        "dataset_folder_path = 'flower_photos'\r\n",
        "\r\n",
        "class DLProgress(tqdm):\r\n",
        "    last_block = 0\r\n",
        "\r\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\r\n",
        "        self.total = total_size\r\n",
        "        self.update((block_num - self.last_block) * block_size)\r\n",
        "        self.last_block = block_num\r\n",
        "\r\n",
        "if not isfile('flower_photos.tar.gz'):\r\n",
        "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Flowers Dataset') as pbar:\r\n",
        "        urlretrieve(\r\n",
        "            'http://download.tensorflow.org/example_images/flower_photos.tgz',\r\n",
        "            'flower_photos.tar.gz',\r\n",
        "            pbar.hook)\r\n",
        "\r\n",
        "if not isdir(dataset_folder_path):\r\n",
        "    with tarfile.open('flower_photos.tar.gz') as tar:\r\n",
        "        tar.extractall()\r\n",
        "        tar.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Flowers Dataset: 229MB [00:05, 43.3MB/s]                          \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FL5u4Coo3ka"
      },
      "source": [
        "## ConvNet Codes\r\n",
        "\r\n",
        "Below, we'll run through all the images in our dataset and get codes for each of them. That is, we'll run the images through the VGGNet convolutional layers and record the values of the first fully connected layer. We can then write these to a file for later when we build our own classifier.\r\n",
        "\r\n",
        "Here we're using the `vgg16` module from `tensorflow_vgg`. The network takes images of size $224 \\times 224 \\times 3$ as input. Then it has 5 sets of convolutional layers. The network implemented here has this structure (copied from [the source code](https://github.com/machrisaa/tensorflow-vgg/blob/master/vgg16.py)):\r\n",
        "\r\n",
        "```\r\n",
        "self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\r\n",
        "self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\r\n",
        "self.pool1 = self.max_pool(self.conv1_2, 'pool1')\r\n",
        "\r\n",
        "self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\r\n",
        "self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\r\n",
        "self.pool2 = self.max_pool(self.conv2_2, 'pool2')\r\n",
        "\r\n",
        "self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\r\n",
        "self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\r\n",
        "self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\r\n",
        "self.pool3 = self.max_pool(self.conv3_3, 'pool3')\r\n",
        "\r\n",
        "self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\r\n",
        "self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\r\n",
        "self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\r\n",
        "self.pool4 = self.max_pool(self.conv4_3, 'pool4')\r\n",
        "\r\n",
        "self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\r\n",
        "self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\r\n",
        "self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\r\n",
        "self.pool5 = self.max_pool(self.conv5_3, 'pool5')\r\n",
        "\r\n",
        "self.fc6 = self.fc_layer(self.pool5, \"fc6\")\r\n",
        "self.relu6 = tf.nn.relu(self.fc6)\r\n",
        "```\r\n",
        "\r\n",
        "So what we want are the values of the first fully connected layer, after being ReLUd (`self.relu6`). To build the network, we use\r\n",
        "\r\n",
        "```\r\n",
        "with tf.Session() as sess:\r\n",
        "    vgg = vgg16.Vgg16()\r\n",
        "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\r\n",
        "    with tf.name_scope(\"content_vgg\"):\r\n",
        "        vgg.build(input_)\r\n",
        "```\r\n",
        "\r\n",
        "This creates the `vgg` object, then builds the graph with `vgg.build(input_)`. Then to get the values from the layer,\r\n",
        "\r\n",
        "```\r\n",
        "feed_dict = {input_: images}\r\n",
        "codes = sess.run(vgg.relu6, feed_dict=feed_dict)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQma8D7hoss4"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow_vgg import vgg16\r\n",
        "from tensorflow_vgg import utils"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhoXxedo2X4"
      },
      "source": [
        "data_dir = 'flower_photos/'\r\n",
        "contents = os.listdir(data_dir)\r\n",
        "classes = [each for each in contents if os.path.isdir(data_dir + each)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGJI8UPKpJNQ"
      },
      "source": [
        "Below I'm running images through the VGG network in batches.\r\n",
        "\r\n",
        "> **Exercise:** Below, build the VGG network. Also get the codes from the first fully connected layer (make sure you get the ReLUd values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd8Ky_oTpD2b",
        "outputId": "37bfbb04-22e3-4e2b-bf81-681b07ebe5d9"
      },
      "source": [
        "# Set the batch size higher if you can fit in in your GPU memory\r\n",
        "batch_size = 10\r\n",
        "codes_list = []\r\n",
        "labels = []\r\n",
        "batch = []\r\n",
        "\r\n",
        "codes = None\r\n",
        "\r\n",
        "with tf.Session() as sess:\r\n",
        "    # TODO: Build the vgg network here\r\n",
        "    vgg = vgg16.Vgg16()\r\n",
        "    input_ = tf.placeholder(tf.float32,[None,224,224,3])\r\n",
        "    with tf.name_scope(\"content_vgg\"):\r\n",
        "        vgg.build(input_)\r\n",
        "\r\n",
        "    for each in classes:\r\n",
        "        print(\"Starting {} images\".format(each))\r\n",
        "        class_path = data_dir + each\r\n",
        "        files = os.listdir(class_path)\r\n",
        "        for ii, file in enumerate(files, 1):\r\n",
        "            # Add images to the current batch\r\n",
        "            # utils.load_image crops the input images for us, from the center\r\n",
        "            img = utils.load_image(os.path.join(class_path, file))\r\n",
        "            batch.append(img.reshape((1, 224, 224, 3)))\r\n",
        "            labels.append(each)\r\n",
        "            \r\n",
        "            # Running the batch through the network to get the codes\r\n",
        "            if ii % batch_size == 0 or ii == len(files):\r\n",
        "                \r\n",
        "                # Image batch to pass to VGG network\r\n",
        "                images = np.concatenate(batch)\r\n",
        "                \r\n",
        "                # TODO: Get the values from the relu6 layer of the VGG network\r\n",
        "                codes_batch = sess.run(vgg.relu6,{input_:images})\r\n",
        "                \r\n",
        "                # Here I'm building an array of the codes\r\n",
        "                if codes is None:\r\n",
        "                    codes = codes_batch\r\n",
        "                else:\r\n",
        "                    codes = np.concatenate((codes, codes_batch))\r\n",
        "                \r\n",
        "                # Reset to start building the next batch\r\n",
        "                batch = []\r\n",
        "                print('{} images processed'.format(ii))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tensorflow_vgg/vgg16.npy\n",
            "npy file loaded\n",
            "build model started\n",
            "build model finished: 1s\n",
            "Starting daisy images\n",
            "10 images processed\n",
            "20 images processed\n",
            "30 images processed\n",
            "40 images processed\n",
            "50 images processed\n",
            "60 images processed\n",
            "70 images processed\n",
            "80 images processed\n",
            "90 images processed\n",
            "100 images processed\n",
            "110 images processed\n",
            "120 images processed\n",
            "130 images processed\n",
            "140 images processed\n",
            "150 images processed\n",
            "160 images processed\n",
            "170 images processed\n",
            "180 images processed\n",
            "190 images processed\n",
            "200 images processed\n",
            "210 images processed\n",
            "220 images processed\n",
            "230 images processed\n",
            "240 images processed\n",
            "250 images processed\n",
            "260 images processed\n",
            "270 images processed\n",
            "280 images processed\n",
            "290 images processed\n",
            "300 images processed\n",
            "310 images processed\n",
            "320 images processed\n",
            "330 images processed\n",
            "340 images processed\n",
            "350 images processed\n",
            "360 images processed\n",
            "370 images processed\n",
            "380 images processed\n",
            "390 images processed\n",
            "400 images processed\n",
            "410 images processed\n",
            "420 images processed\n",
            "430 images processed\n",
            "440 images processed\n",
            "450 images processed\n",
            "460 images processed\n",
            "470 images processed\n",
            "480 images processed\n",
            "490 images processed\n",
            "500 images processed\n",
            "510 images processed\n",
            "520 images processed\n",
            "530 images processed\n",
            "540 images processed\n",
            "550 images processed\n",
            "560 images processed\n",
            "570 images processed\n",
            "580 images processed\n",
            "590 images processed\n",
            "600 images processed\n",
            "610 images processed\n",
            "620 images processed\n",
            "630 images processed\n",
            "633 images processed\n",
            "Starting roses images\n",
            "10 images processed\n",
            "20 images processed\n",
            "30 images processed\n",
            "40 images processed\n",
            "50 images processed\n",
            "60 images processed\n",
            "70 images processed\n",
            "80 images processed\n",
            "90 images processed\n",
            "100 images processed\n",
            "110 images processed\n",
            "120 images processed\n",
            "130 images processed\n",
            "140 images processed\n",
            "150 images processed\n",
            "160 images processed\n",
            "170 images processed\n",
            "180 images processed\n",
            "190 images processed\n",
            "200 images processed\n",
            "210 images processed\n",
            "220 images processed\n",
            "230 images processed\n",
            "240 images processed\n",
            "250 images processed\n",
            "260 images processed\n",
            "270 images processed\n",
            "280 images processed\n",
            "290 images processed\n",
            "300 images processed\n",
            "310 images processed\n",
            "320 images processed\n",
            "330 images processed\n",
            "340 images processed\n",
            "350 images processed\n",
            "360 images processed\n",
            "370 images processed\n",
            "380 images processed\n",
            "390 images processed\n",
            "400 images processed\n",
            "410 images processed\n",
            "420 images processed\n",
            "430 images processed\n",
            "440 images processed\n",
            "450 images processed\n",
            "460 images processed\n",
            "470 images processed\n",
            "480 images processed\n",
            "490 images processed\n",
            "500 images processed\n",
            "510 images processed\n",
            "520 images processed\n",
            "530 images processed\n",
            "540 images processed\n",
            "550 images processed\n",
            "560 images processed\n",
            "570 images processed\n",
            "580 images processed\n",
            "590 images processed\n",
            "600 images processed\n",
            "610 images processed\n",
            "620 images processed\n",
            "630 images processed\n",
            "640 images processed\n",
            "641 images processed\n",
            "Starting tulips images\n",
            "10 images processed\n",
            "20 images processed\n",
            "30 images processed\n",
            "40 images processed\n",
            "50 images processed\n",
            "60 images processed\n",
            "70 images processed\n",
            "80 images processed\n",
            "90 images processed\n",
            "100 images processed\n",
            "110 images processed\n",
            "120 images processed\n",
            "130 images processed\n",
            "140 images processed\n",
            "150 images processed\n",
            "160 images processed\n",
            "170 images processed\n",
            "180 images processed\n",
            "190 images processed\n",
            "200 images processed\n",
            "210 images processed\n",
            "220 images processed\n",
            "230 images processed\n",
            "240 images processed\n",
            "250 images processed\n",
            "260 images processed\n",
            "270 images processed\n",
            "280 images processed\n",
            "290 images processed\n",
            "300 images processed\n",
            "310 images processed\n",
            "320 images processed\n",
            "330 images processed\n",
            "340 images processed\n",
            "350 images processed\n",
            "360 images processed\n",
            "370 images processed\n",
            "380 images processed\n",
            "390 images processed\n",
            "400 images processed\n",
            "410 images processed\n",
            "420 images processed\n",
            "430 images processed\n",
            "440 images processed\n",
            "450 images processed\n",
            "460 images processed\n",
            "470 images processed\n",
            "480 images processed\n",
            "490 images processed\n",
            "500 images processed\n",
            "510 images processed\n",
            "520 images processed\n",
            "530 images processed\n",
            "540 images processed\n",
            "550 images processed\n",
            "560 images processed\n",
            "570 images processed\n",
            "580 images processed\n",
            "590 images processed\n",
            "600 images processed\n",
            "610 images processed\n",
            "620 images processed\n",
            "630 images processed\n",
            "640 images processed\n",
            "650 images processed\n",
            "660 images processed\n",
            "670 images processed\n",
            "680 images processed\n",
            "690 images processed\n",
            "700 images processed\n",
            "710 images processed\n",
            "720 images processed\n",
            "730 images processed\n",
            "740 images processed\n",
            "750 images processed\n",
            "760 images processed\n",
            "770 images processed\n",
            "780 images processed\n",
            "790 images processed\n",
            "799 images processed\n",
            "Starting sunflowers images\n",
            "10 images processed\n",
            "20 images processed\n",
            "30 images processed\n",
            "40 images processed\n",
            "50 images processed\n",
            "60 images processed\n",
            "70 images processed\n",
            "80 images processed\n",
            "90 images processed\n",
            "100 images processed\n",
            "110 images processed\n",
            "120 images processed\n",
            "130 images processed\n",
            "140 images processed\n",
            "150 images processed\n",
            "160 images processed\n",
            "170 images processed\n",
            "180 images processed\n",
            "190 images processed\n",
            "200 images processed\n",
            "210 images processed\n",
            "220 images processed\n",
            "230 images processed\n",
            "240 images processed\n",
            "250 images processed\n",
            "260 images processed\n",
            "270 images processed\n",
            "280 images processed\n",
            "290 images processed\n",
            "300 images processed\n",
            "310 images processed\n",
            "320 images processed\n",
            "330 images processed\n",
            "340 images processed\n",
            "350 images processed\n",
            "360 images processed\n",
            "370 images processed\n",
            "380 images processed\n",
            "390 images processed\n",
            "400 images processed\n",
            "410 images processed\n",
            "420 images processed\n",
            "430 images processed\n",
            "440 images processed\n",
            "450 images processed\n",
            "460 images processed\n",
            "470 images processed\n",
            "480 images processed\n",
            "490 images processed\n",
            "500 images processed\n",
            "510 images processed\n",
            "520 images processed\n",
            "530 images processed\n",
            "540 images processed\n",
            "550 images processed\n",
            "560 images processed\n",
            "570 images processed\n",
            "580 images processed\n",
            "590 images processed\n",
            "600 images processed\n",
            "610 images processed\n",
            "620 images processed\n",
            "630 images processed\n",
            "640 images processed\n",
            "650 images processed\n",
            "660 images processed\n",
            "670 images processed\n",
            "680 images processed\n",
            "690 images processed\n",
            "699 images processed\n",
            "Starting dandelion images\n",
            "10 images processed\n",
            "20 images processed\n",
            "30 images processed\n",
            "40 images processed\n",
            "50 images processed\n",
            "60 images processed\n",
            "70 images processed\n",
            "80 images processed\n",
            "90 images processed\n",
            "100 images processed\n",
            "110 images processed\n",
            "120 images processed\n",
            "130 images processed\n",
            "140 images processed\n",
            "150 images processed\n",
            "160 images processed\n",
            "170 images processed\n",
            "180 images processed\n",
            "190 images processed\n",
            "200 images processed\n",
            "210 images processed\n",
            "220 images processed\n",
            "230 images processed\n",
            "240 images processed\n",
            "250 images processed\n",
            "260 images processed\n",
            "270 images processed\n",
            "280 images processed\n",
            "290 images processed\n",
            "300 images processed\n",
            "310 images processed\n",
            "320 images processed\n",
            "330 images processed\n",
            "340 images processed\n",
            "350 images processed\n",
            "360 images processed\n",
            "370 images processed\n",
            "380 images processed\n",
            "390 images processed\n",
            "400 images processed\n",
            "410 images processed\n",
            "420 images processed\n",
            "430 images processed\n",
            "440 images processed\n",
            "450 images processed\n",
            "460 images processed\n",
            "470 images processed\n",
            "480 images processed\n",
            "490 images processed\n",
            "500 images processed\n",
            "510 images processed\n",
            "520 images processed\n",
            "530 images processed\n",
            "540 images processed\n",
            "550 images processed\n",
            "560 images processed\n",
            "570 images processed\n",
            "580 images processed\n",
            "590 images processed\n",
            "600 images processed\n",
            "610 images processed\n",
            "620 images processed\n",
            "630 images processed\n",
            "640 images processed\n",
            "650 images processed\n",
            "660 images processed\n",
            "670 images processed\n",
            "680 images processed\n",
            "690 images processed\n",
            "700 images processed\n",
            "710 images processed\n",
            "720 images processed\n",
            "730 images processed\n",
            "740 images processed\n",
            "750 images processed\n",
            "760 images processed\n",
            "770 images processed\n",
            "780 images processed\n",
            "790 images processed\n",
            "800 images processed\n",
            "810 images processed\n",
            "820 images processed\n",
            "830 images processed\n",
            "840 images processed\n",
            "850 images processed\n",
            "860 images processed\n",
            "870 images processed\n",
            "880 images processed\n",
            "890 images processed\n",
            "898 images processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgEGUDFzpN6k"
      },
      "source": [
        "# write codes to file\r\n",
        "with open('codes', 'w') as f:\r\n",
        "    codes.tofile(f)\r\n",
        "    \r\n",
        "# write labels to file\r\n",
        "import csv\r\n",
        "with open('labels', 'w') as f:\r\n",
        "    writer = csv.writer(f, delimiter='\\n')\r\n",
        "    writer.writerow(labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrv9M2jiqpgX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}